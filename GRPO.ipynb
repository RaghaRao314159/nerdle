{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc24c67",
      "metadata": {},
      "outputs": [],
      "source": [
        "# training model\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# plotting and saving models\n",
        "# import matplotlib.pyplot as plt\n",
        "# import wandb # we can start saving models inside wandb when they get good enough\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c5605c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# BASE model\n",
        "# Option 1: Use base Unsloth model\n",
        "BASE_HUGGINGFACE_DIRECTORY = \"unsloth\"\n",
        "# BASE_MODEL = \"Qwen3-4B\"\n",
        "BASE_MODEL = \"Qwen3-4B-unsloth-bnb-4bit\"\n",
        "USE_SAVED_MODEL = True  # Set to True to use a previously trained model\n",
        "\n",
        "# Option 2: Use previously trained model (set USE_SAVED_MODEL = True)\n",
        "# BASE_HUGGINGFACE_DIRECTORY = None  # Set to None for local models\n",
        "# BASE_MODEL = \"saved_models/GRPO-Qwen3-4B_rank16_all_rewards_epoch_5_4bit\"\n",
        "\n",
        "MAX_SEQ_LENGTH = 4096 # base model\n",
        "\n",
        "# Dataset\n",
        "DATASET_PATH = \"dataset_nerdle_2_history.jsonl\"\n",
        "TRAINSET_SIZE = 500\n",
        "TESTSET_SIZE = 2\n",
        "\n",
        "# Training\n",
        "LORA_RANK = 16\n",
        "GPU_MEMORY_UTILIZATION = 0.3\n",
        "MAX_PROMPT_LENGTH = 4096\n",
        "MAX_COMPLETION_LENGTH = 4096\n",
        "EPOCHS = 2\n",
        "USE_VLLM = True\n",
        "\n",
        "# SAVING MODEL\n",
        "# from huggingface_hub import login\n",
        "# HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "# login(HUGGINGFACE_TOKEN)\n",
        "\n",
        "SAVE_HUGGINGFACE_DIRECTORY = \"RaghaRao314159\"\n",
        "# SAVE_MODEL_NAME = \"GRPO-Qwen3-4B\"\n",
        "SAVE_MODEL_NAME = \"GRPO-Qwen3-4B\"\n",
        "EXPERIMENT_DESCRIPTION = \"2_history\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c39bffd",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_INSTRUCTION = f\"\"\"\n",
        "Give the final equation (and only the equation) on a new line as follows: ###[Your final answer].\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1cbb6a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_SAVED_MODEL:\n",
        "    # Load a previously saved/merged model (already quantized)\n",
        "    model_path = BASE_MODEL  # Should be a local path like \"saved_models/...\"\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = model_path,\n",
        "        max_seq_length = MAX_SEQ_LENGTH,\n",
        "        load_in_4bit = False,  # Already quantized\n",
        "        fast_inference = False,  # Disable vLLM for loading saved models\n",
        "        max_lora_rank = LORA_RANK,\n",
        "        gpu_memory_utilization = GPU_MEMORY_UTILIZATION,\n",
        "    )\n",
        "else:\n",
        "    # Load base model from HuggingFace\n",
        "    model_path = f\"{BASE_HUGGINGFACE_DIRECTORY}/{BASE_MODEL}\"\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = model_path,\n",
        "        max_seq_length = MAX_SEQ_LENGTH,\n",
        "        load_in_4bit = True,  # False for LoRA 16bit\n",
        "        fast_inference = USE_VLLM,  # Enable vLLM fast inference\n",
        "        max_lora_rank = LORA_RANK,  # 8, 16, 32, ... (the larger the rank, the more memory it uses)\n",
        "        gpu_memory_utilization = GPU_MEMORY_UTILIZATION,  # Reduce if out of memory\n",
        "    )\n",
        "\n",
        "# Disable \"think\" by monkey-patching the tokenizer’s chat template\n",
        "orig_apply = tokenizer.apply_chat_template\n",
        "def apply_no_think(messages, **kwargs):\n",
        "    return orig_apply(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,      # ← hard-disable chain-of-thought\n",
        "        **{k: v for k, v in kwargs.items() if k not in (\"tokenize\",\"add_generation_prompt\")}\n",
        "    )\n",
        "tokenizer.apply_chat_template = apply_no_think\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_RANK,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],  # Remove QKVO if out of memory\n",
        "    lora_alpha=LORA_RANK,  # scaling parameter for delta W (LoRA) = alpha/rank ---> set same as rank so no need to retune for different rank\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Enable long context finetuning\n",
        "    # use_gradient_checkpointing= False, # Disable gradient checkpointing for faster training\n",
        "    random_state=3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3404b8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# format of dataset:\n",
        "# {\"conversations\": [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": answer}]}\n",
        "# {\"conversations\": [{\"role\": \"user\", \"content\": prompt2}, {\"role\": \"assistant\", \"content\": answer2}]} ...\n",
        "dataset_original = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\").select(\n",
        "    range(TRAINSET_SIZE)\n",
        ")  # select first 1000 rows for training\n",
        "\n",
        "\n",
        "# def formatting_prompts_func(example):\n",
        "#     # if number of tokens exceed max prompt length, do not include the prompt\n",
        "#     if len(tokenizer.encode(example[\"conversations\"][0][\"content\"])) < 10000:\n",
        "#         return {\n",
        "#             \"prompt\": [\n",
        "#                 {\"role\": \"user\", \"content\": SYSTEM_INSTRUCTION + \"\\n\" + example[\"conversations\"][0][\"content\"]}\n",
        "#             ],\n",
        "#             \"answer\": example[\"conversations\"][1][\"content\"]\n",
        "#         }\n",
        "#     else:\n",
        "#         # do not add this row to the new dataset object\n",
        "#         return None\n",
        "\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    # if number of tokens exceed max prompt length, do not include the prompt\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": SYSTEM_INSTRUCTION\n",
        "                + \"\\n\"\n",
        "                + example[\"conversations\"][0][\"content\"],\n",
        "            }\n",
        "        ],\n",
        "        \"answer\": example[\"conversations\"][1][\"content\"],\n",
        "    }\n",
        "\n",
        "\n",
        "def is_short_enough(example):\n",
        "    return (\n",
        "        len(tokenizer.encode(example[\"conversations\"][0][\"content\"])) < 4096 - 150\n",
        "    ) and (len(tokenizer.encode(example[\"conversations\"][1][\"content\"])) < 4096 - 150)\n",
        "\n",
        "\n",
        "filtered_dataset = dataset_original.filter(is_short_enough)\n",
        "\n",
        "dataset_split = filtered_dataset.train_test_split(test_size=TESTSET_SIZE, seed=42)\n",
        "dataset_mapped = dataset_split.map(\n",
        "    formatting_prompts_func, remove_columns=[\"conversations\"]\n",
        ")\n",
        "# dataset_split = dataset_original.train_test_split(test_size=TESTSET_SIZE, seed=42)\n",
        "# dataset_mapped = dataset_split.map(formatting_prompts_func, remove_columns=[\"conversations\"])\n",
        "print(\"\\nOriginal dataset\\n\", dataset_original)\n",
        "print(\"\\nMapped dataset\\n\", dataset_mapped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d63445a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def reward_fn(prompts, completions, answer, **kwargs):\n",
        "    reward = []\n",
        "    for prompt, completion, ans in zip(prompts, completions, answer):\n",
        "        llm_input = prompt[0]['content']\n",
        "        llm_output = completion[0]['content']\n",
        "        correct_answer = ans\n",
        "        \n",
        "        # Parse the output by removing ###, [, and ]\n",
        "        parsed_output = llm_output\n",
        "        # Remove ### prefix\n",
        "        parsed_output = re.sub(r'###', '', parsed_output)\n",
        "        # Remove [ and ] brackets\n",
        "        parsed_output = re.sub(r'[\\[\\]]', '', parsed_output)\n",
        "        # Strip whitespace\n",
        "        parsed_output = parsed_output.strip()\n",
        "        \n",
        "        # Check if equation is valid length (8 characters for Nerdle)\n",
        "        if len(parsed_output) != 8:\n",
        "            reward.append(-24)\n",
        "            continue\n",
        "        \n",
        "        # Check if it's a valid math equation\n",
        "        try:\n",
        "            # Split on = sign\n",
        "            if '=' not in parsed_output:\n",
        "                reward.append(-24)\n",
        "                continue\n",
        "            \n",
        "            left_side, right_side = parsed_output.split('=')\n",
        "            \n",
        "            # Evaluate left side\n",
        "            left_result = eval(left_side)\n",
        "            right_result = eval(right_side)\n",
        "            \n",
        "            # Check if equation is mathematically correct\n",
        "            if abs(left_result - right_result) > 0.0001:  # Allow small floating point errors\n",
        "                reward.append(-24)\n",
        "                continue\n",
        "                \n",
        "        except Exception as e:\n",
        "            # Invalid math equation\n",
        "            reward.append(-24)\n",
        "            continue\n",
        "        \n",
        "        # Calculate Nerdle-style score\n",
        "        score = 0\n",
        "        correct_answer_clean = correct_answer.strip()\n",
        "        \n",
        "        # Count character matches\n",
        "        for i, char in enumerate(parsed_output):\n",
        "            if i < len(correct_answer_clean):\n",
        "                if char == correct_answer_clean[i]:\n",
        "                    # Correct character in correct position\n",
        "                    score += 3\n",
        "                elif char in correct_answer_clean:\n",
        "                    # Correct character in wrong position\n",
        "                    score += 1\n",
        "                # else: character not in correct equation = 0 points\n",
        "        \n",
        "        reward.append(score)\n",
        "        print(f\"Output: {parsed_output}, Correct: {correct_answer_clean}, Score: {score}\")\n",
        "    \n",
        "    return reward\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e850c77",
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = GRPOConfig(\n",
        "    use_vllm=USE_VLLM,  # use vLLM for fast inference!\n",
        "    learning_rate=5e-6,  # learning rate for gradient descent\n",
        "    adam_beta1=0.9,  # beta1 for adamw optimizer\n",
        "    adam_beta2=0.99,  # beta2 for adamw optimizer\n",
        "    weight_decay=0.1,  # weight decay for adamw optimizer\n",
        "    warmup_ratio=0.1,  # Gradually increases learning rate at the start\n",
        "    lr_scheduler_type=\"cosine\",  # After warmup, does cosine annealing on learning rate\n",
        "    optim=\"adamw_8bit\",  # Quantisation of adam update steps. This is fine and doesnt have huge impact on accuracy\n",
        "    per_device_train_batch_size=4,  # batch size per device\n",
        "    gradient_accumulation_steps=2,  # effective batch size is 256 = 64 * 4\n",
        "    num_generations=8,  # Number of outputs that GRPO produces to estimate the advantage\n",
        "    logging_steps=1,  # log every 4 steps\n",
        "    bf16=is_bfloat16_supported(),  # use bfloat16 for faster training\n",
        "    fp16=not is_bfloat16_supported(),  # if bf16 is not supported, use fp16\n",
        "    max_prompt_length=MAX_PROMPT_LENGTH,  # max length of the prompt\n",
        "    max_completion_length=MAX_COMPLETION_LENGTH,  # max length of the completion\n",
        "    num_train_epochs=EPOCHS,  # number of times to train over whole dataset\n",
        "    # max_steps = 1, # max number of steps\n",
        "    # save_steps = 250, # save every 250 steps\n",
        "    max_grad_norm=0.1,  # max gradient norm clipping to prevent exploding gradients\n",
        "    report_to=\"none\",  # Can use Weights & Biases\n",
        "    output_dir=\"outputs\",\n",
        "    loss_type=\"bnpo\",  # or \"grpo\" or \"dr_grpo\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6358769a",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[reward_fn],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_mapped[\"train\"],\n",
        "    # eval_dataset = dataset_mapped[\"test\"],\n",
        ")\n",
        "# wandb.init(project=WEIGHTS_AND_BIASES_PROJECT_NAME,\n",
        "#            name=f\"{SAVE_MODEL_NAME}_rank{LORA_RANK}_epochs{EPOCHS}_{EXPERIMENT_DESCRIPTION}\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7892d2a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model locally with merged LoRA adapters\n",
        "LOCAL_SAVE_DIR = f\"saved_models/{SAVE_MODEL_NAME}_rank{LORA_RANK}_{EXPERIMENT_DESCRIPTION}\"\n",
        "\n",
        "# Save merged model in 16-bit format\n",
        "# model.save_pretrained_merged(LOCAL_SAVE_DIR, tokenizer, save_method=\"merged_16bit\")\n",
        "print(f\"Model saved locally to: {LOCAL_SAVE_DIR}\")\n",
        "\n",
        "# Optionally, you can also save in 4-bit quantized format to save space:\n",
        "model.save_pretrained_merged(f\"{LOCAL_SAVE_DIR}_4bit\", tokenizer, save_method=\"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e035c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.push_to_hub_merged(\n",
        "#     f\"{SAVE_HUGGINGFACE_DIRECTORY}/{SAVE_MODEL_NAME}_rank{LORA_RANK}_{EXPERIMENT_DESCRIPTION}\",\n",
        "#     tokenizer,\n",
        "#     save_method=\"merged_16bit\",\n",
        "#     token=HUGGINGFACE_TOKEN,\n",
        "# )\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
