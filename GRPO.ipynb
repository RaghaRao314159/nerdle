{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc24c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# plotting and saving models\n",
    "import matplotlib.pyplot as plt\n",
    "# import wandb # we can start saving models inside wandb when they get good enough\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom rewards\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from rewards.RewardCombined import RewardCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4f8840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = \"../datasets/first-dataset/lottie-dataset.jsonl.zip\"\n",
    "extract_dir = \"../datasets/\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bASE model\n",
    "BASE_HUGGINGFACE_DIRECTORY = \"unsloth\"\n",
    "# BASE_MODEL = \"Qwen3-4B\"\n",
    "BASE_MODEL = \"Qwen3-14B-unsloth-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 4096 # base model\n",
    "\n",
    "# Dataset\n",
    "DATASET_PATH = \"../datasets/lottie-dataset.jsonl\"\n",
    "TRAINSET_SIZE = 1000\n",
    "TESTSET_SIZE = 8\n",
    "\n",
    "# Training\n",
    "LORA_RANK = 16\n",
    "GPU_MEMORY_UTILIZATION = 0.3\n",
    "MAX_PROMPT_LENGTH = 4096\n",
    "MAX_COMPLETION_LENGTH = 4096\n",
    "EPOCHS = 10\n",
    "USE_VLLM = True\n",
    "\n",
    "# SAVING MODEL\n",
    "from huggingface_hub import login\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(HUGGINGFACE_TOKEN)\n",
    "\n",
    "SAVE_HUGGINGFACE_DIRECTORY = \"RaghaRao314159\"\n",
    "# SAVE_MODEL_NAME = \"GRPO-Qwen3-4B\"\n",
    "SAVE_MODEL_NAME = \"Qwen3-14B-unsloth-bnb-4bit\"\n",
    "EXPERIMENT_DESCRIPTION = \"all_rewards_epoch_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = f\"{BASE_HUGGINGFACE_DIRECTORY}/{BASE_MODEL}\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = USE_VLLM, # Enable vLLM fast inference\n",
    "    max_lora_rank = LORA_RANK, # 8, 16, 32, ... (the larger the rank, the more memory it uses)\n",
    "    gpu_memory_utilization = GPU_MEMORY_UTILIZATION, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "# Disable \"think\" by monkey-patching the tokenizer’s chat template\n",
    "# orig_apply = tokenizer.apply_chat_template\n",
    "# def apply_no_think(messages, **kwargs):\n",
    "#     return orig_apply(\n",
    "#         messages,\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True,\n",
    "#         enable_thinking=False,      # ← hard-disable chain-of-thought\n",
    "#         **{k: v for k, v in kwargs.items() if k not in (\"tokenize\",\"add_generation_prompt\")}\n",
    "#     )\n",
    "# tokenizer.apply_chat_template = apply_no_think\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_RANK, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = LORA_RANK, # scaling parameter for delta W (LoRA) = alpha/rank ---> set same as rank so no need to retune for different rank\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    # use_gradient_checkpointing= False, # Disable gradient checkpointing for faster training\n",
    "    random_state = 3407,\n",
    ")                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3404b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# format of dataset: \n",
    "# {\"conversations\": [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": answer}]}\n",
    "# {\"conversations\": [{\"role\": \"user\", \"content\": prompt2}, {\"role\": \"assistant\", \"content\": answer2}]} ...\n",
    "dataset_original = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\").select(range(TRAINSET_SIZE)) # select first 1000 rows for training\n",
    "\n",
    "SYSTEM_INSTRUCTION = f\"\"\"\n",
    "You are an expert static to animated vector generation agent.\n",
    "You will be given an SVG file and you will generate a Lottie animation from it in json format.\n",
    "Follow the Lottie schema, keeping in mind that there can be multiple layers and assets.\n",
    "\n",
    "The SVG file is:\n",
    "\"\"\"\n",
    "\n",
    "# def formatting_prompts_func(example):\n",
    "#     # if number of tokens exceed max prompt length, do not include the prompt\n",
    "#     if len(tokenizer.encode(example[\"conversations\"][0][\"content\"])) < 10000:\n",
    "#         return {\n",
    "#             \"prompt\": [\n",
    "#                 {\"role\": \"user\", \"content\": SYSTEM_INSTRUCTION + \"\\n\" + example[\"conversations\"][0][\"content\"]}\n",
    "#             ],\n",
    "#             \"answer\": example[\"conversations\"][1][\"content\"]\n",
    "#         }\n",
    "#     else:\n",
    "#         # do not add this row to the new dataset object\n",
    "#         return None\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    # if number of tokens exceed max prompt length, do not include the prompt\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"user\", \"content\": SYSTEM_INSTRUCTION + \"\\n\" + example[\"conversations\"][0][\"content\"]}\n",
    "        ],\n",
    "        \"answer\": example[\"conversations\"][1][\"content\"]\n",
    "    }\n",
    "\n",
    "def is_short_enough(example):\n",
    "    return (len(tokenizer.encode(example[\"conversations\"][0][\"content\"])) < 4096 - 150\n",
    "    ) and (len(tokenizer.encode(example[\"conversations\"][1][\"content\"])) < 4096 - 150)\n",
    "filtered_dataset = dataset_original.filter(is_short_enough)\n",
    "\n",
    "dataset_split = filtered_dataset.train_test_split(test_size=TESTSET_SIZE, seed=42)\n",
    "dataset_mapped = dataset_split.map(formatting_prompts_func, remove_columns=[\"conversations\"])\n",
    "# dataset_split = dataset_original.train_test_split(test_size=TESTSET_SIZE, seed=42)\n",
    "# dataset_mapped = dataset_split.map(formatting_prompts_func, remove_columns=[\"conversations\"])\n",
    "print(\"\\nOriginal dataset\\n\", dataset_original)\n",
    "print(\"\\nMapped dataset\\n\", dataset_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf641621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define, for each algorithm, how to pull out each text field\n",
    "accessors = {\n",
    "    \"GRPO\": {\n",
    "        \"prompt\":  lambda x: x[\"prompt\"][0][\"content\"],\n",
    "        \"answer\":  lambda x: x[\"answer\"],\n",
    "    },\n",
    "    \"ORPO\": {\n",
    "        \"prompt\":   lambda x: x[\"prompt\"],\n",
    "        \"chosen\":   lambda x: x[\"chosen\"],\n",
    "        \"rejected\": lambda x: x[\"rejected\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# pick the right fields for this run\n",
    "fields = accessors[\"GRPO\"]\n",
    "\n",
    "def get_stats(tok_lengths, threshold):\n",
    "    # find the proportion of prompts that are below the threshold\n",
    "    below_threshold = sum(1 for length in tok_lengths if length < threshold)\n",
    "    proportion_below_threshold = below_threshold / len(tok_lengths) if tok_lengths else 0\n",
    "\n",
    "    return proportion_below_threshold\n",
    "\n",
    "def compute_and_print_stats(name, fn):\n",
    "    # first example\n",
    "    sample = fn(dataset_mapped[\"train\"][0])\n",
    "    print(f\"Token count in first {name}:\",      len(tokenizer.encode(sample)))\n",
    "    print()\n",
    "\n",
    "    # collect across the whole train set\n",
    "    all_texts    = [fn(item) for item in dataset_mapped[\"train\"]]\n",
    "    tok_lengths  = [len(tokenizer.encode(txt)) for txt in all_texts]\n",
    "\n",
    "    print(f\"Max {name} count in tokens:  {max(tok_lengths)}\")\n",
    "    print(f\"Min {name} count in tokens:  {min(tok_lengths)}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    return {\n",
    "        f\"{name}\": tok_lengths\n",
    "    }\n",
    "\n",
    "# print stats on the dataset\n",
    "tok_stats = {}\n",
    "for field_name, accessor_fn in fields.items():\n",
    "    tok_stats.update(compute_and_print_stats(field_name, accessor_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c13dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"for lottie file stats\")\n",
    "\n",
    "get_stats(tok_stats[\"prompt\"], 4096)\n",
    "get_stats(tok_stats[\"answer\"], 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e850c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "        use_vllm = USE_VLLM, # use vLLM for fast inference!\n",
    "        learning_rate = 5e-6, # learning rate for gradient descent\n",
    "        adam_beta1 = 0.9, # beta1 for adamw optimizer\n",
    "        adam_beta2 = 0.99, # beta2 for adamw optimizer\n",
    "        weight_decay = 0.1, # weight decay for adamw optimizer\n",
    "        warmup_ratio = 0.1, # Gradually increases learning rate at the start\n",
    "        lr_scheduler_type = \"cosine\", # After warmup, does cosine annealing on learning rate\n",
    "        optim = \"adamw_8bit\", # Quantisation of adam update steps. This is fine and doesnt have huge impact on accuracy\n",
    "        per_device_train_batch_size = 4, # batch size per device\n",
    "        gradient_accumulation_steps = 2, # effective batch size is 256 = 64 * 4\n",
    "        num_generations = 8, # Number of outputs that GRPO produces to estimate the advantage\n",
    "        logging_steps = 1, # log every 4 steps\n",
    "        bf16 = is_bfloat16_supported(), # use bfloat16 for faster training\n",
    "        fp16 = not is_bfloat16_supported(), # if bf16 is not supported, use fp16\n",
    "        max_prompt_length = MAX_PROMPT_LENGTH, # max length of the prompt\n",
    "        max_completion_length = MAX_COMPLETION_LENGTH, # max length of the completion\n",
    "        num_train_epochs = EPOCHS, # number of times to train over whole dataset\n",
    "        # max_steps = 1, # max number of steps\n",
    "        # save_steps = 250, # save every 250 steps\n",
    "        max_grad_norm = 0.1, # max gradient norm clipping to prevent exploding gradients\n",
    "        report_to = \"none\", # Can use Weights & Biases\n",
    "        output_dir = \"outputs\",\n",
    "        loss_type = \"bnpo\", # or \"grpo\" or \"dr_grpo\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc18c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_fn = RewardCombined().reward_grpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "        model = model,\n",
    "        processing_class = tokenizer,\n",
    "        reward_funcs = [\n",
    "            reward_fn\n",
    "        ],\n",
    "        args = training_args,\n",
    "        train_dataset = dataset_mapped[\"train\"],\n",
    "        # eval_dataset = dataset_mapped[\"test\"],\n",
    "    )\n",
    "# wandb.init(project=WEIGHTS_AND_BIASES_PROJECT_NAME,\n",
    "#            name=f\"{SAVE_MODEL_NAME}_rank{LORA_RANK}_epochs{EPOCHS}_{EXPERIMENT_DESCRIPTION}\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfcd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the training data\n",
    "logs = trainer.state.log_history\n",
    "\n",
    "# save the training logs\n",
    "df = pd.DataFrame(logs)\n",
    "df.to_csv(f\"../logs/{SAVE_MODEL_NAME}_rank{LORA_RANK}_{EXPERIMENT_DESCRIPTION}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e035c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_merged(f\"{SAVE_HUGGINGFACE_DIRECTORY}/{SAVE_MODEL_NAME}_rank{LORA_RANK}_{EXPERIMENT_DESCRIPTION}\", tokenizer, save_method = \"merged_16bit\", token=HUGGINGFACE_TOKEN)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
