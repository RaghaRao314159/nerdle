{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc24c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[torchao|WARNING]Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 15:07:27 [__init__.py:216] Automatically detected platform cuda.\n",
      "WARNING 11-02 15:07:27 [interface.py:381] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# plotting and saving models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import wandb # we can start saving models inside wandb when they get good enough\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5605c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd6e2eedf334c46a20ffdde4acc60c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bASE model\n",
    "BASE_HUGGINGFACE_DIRECTORY = \"unsloth\"\n",
    "BASE_MODEL = \"unsloth/Qwen3-4B-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 4096  # base model\n",
    "\n",
    "# Dataset\n",
    "DATASET_PATH = \"../datasets/dataset.jsonl\"\n",
    "TRAINSET_SIZE = 4\n",
    "TESTSET_SIZE = 2\n",
    "\n",
    "# Training\n",
    "LORA_RANK = 16\n",
    "GPU_MEMORY_UTILIZATION = 0.3\n",
    "MAX_PROMPT_LENGTH = 4096\n",
    "MAX_COMPLETION_LENGTH = 4096\n",
    "EPOCHS = 10\n",
    "USE_VLLM = True\n",
    "\n",
    "# SAVING MODEL\n",
    "from huggingface_hub import login\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(HUGGINGFACE_TOKEN)\n",
    "\n",
    "SAVE_HUGGINGFACE_DIRECTORY = \"RaghaRao314159\"\n",
    "SAVE_MODEL_NAME = \"GRPO-Qwen3-4B-bnb-4bit\"\n",
    "EXPERIMENT_DESCRIPTION = \"all_rewards_epoch_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1cbb6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 15:09:33 [vllm_utils.py:694] Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Could not patch vLLM V0 graph capture: No module named 'vllm.worker.model_runner'\n",
      "==((====))==  Unsloth 2025.10.12: Fast Qwen3 patching. Transformers: 4.57.1. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Your GPU cannot handle sequence lengths of 256 due to limited GPU memory.\n",
      "Unsloth: Your GPU can only handle approximately the maximum sequence length of 256.\n",
      "Unsloth: vLLM loading unsloth/Qwen3-4B-unsloth-bnb-4bit with actual GPU utilization = 19.92%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 8.0 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.0 GB. Also swap space = 0 GB.\n",
      "WARNING 11-02 15:09:37 [compilation.py:475] full_cuda_graph is deprecated, use cudagraph_mode=FULL instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 11-02 15:09:37 [utils.py:233] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 256, 'enable_prefix_caching': True, 'swap_space': 0, 'gpu_memory_utilization': 0.1991801589300845, 'max_num_batched_tokens': 2048, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":22,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/Qwen3-4B-unsloth-bnb-4bit'}\n",
      "INFO 11-02 15:09:38 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-02 15:09:38 [model.py:1510] Using max model len 256\n",
      "INFO 11-02 15:09:38 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 11-02 15:09:38 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.4.mlp', 'model.layers.6.self_attn', 'model.layers.34.self_attn', 'model.layers.33.self_attn', 'model.layers.4.self_attn', 'model.layers.34.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.0.mlp', 'model.layers.0.self_attn', 'model.layers.3.mlp', 'model.layers.6.mlp'], 'llm_int8_threshold': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6468af0394ce4216995ef94f2ed50567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863ea31c9bd14f04b5d3dea3a05cb4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c2ef271bb04172b7d34b2dc2545af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6b9f13229347169b671acea91eda16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77296cca3934fbb969bd6634848d13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818c2b9fa869431d82ce3c40001edc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f4cd8adf1746f0a5de3aa49e34e168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c105ed020e74075bc1ecf9718991d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 15:09:42 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='unsloth/Qwen3-4B-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/Qwen3-4B-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen3-4B-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":22,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n",
      "INFO 11-02 15:09:43 [gpu_model_runner.py:2602] Starting to load model unsloth/Qwen3-4B-unsloth-bnb-4bit...\n",
      "INFO 11-02 15:09:44 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "INFO 11-02 15:09:44 [bitsandbytes_loader.py:759] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 11-02 15:09:45 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fb17f108cd4bbc8b6cc9d28613cfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 15:10:42 [weight_utils.py:413] Time spent downloading weights for unsloth/Qwen3-4B-unsloth-bnb-4bit: 56.978722 seconds\n",
      "INFO 11-02 15:10:42 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cc80983a0b45ad92cf4eb233aa2454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e13367867bd4dc087c65b1b4181b071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 15:10:43 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 11-02 15:10:45 [gpu_model_runner.py:2653] Model loading took 3.3795 GiB and 59.483987 seconds\n",
      "INFO 11-02 15:10:57 [backends.py:548] Using cache directory: /home/flowingpurplecrane/.cache/vllm/torch_compile_cache/bbda8cd23c/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 11-02 15:10:57 [backends.py:559] Dynamo bytecode transform time: 11.03 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmp6415_gei/cuda_utils.c:5:10: fatal error: Python.h: No such file or directory\n",
      "    5 | #include <Python.h>\n",
      "      |          ^~~~~~~~~~\n",
      "compilation terminated.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp6415_gei/cuda_utils.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmp6415_gei/cuda_utils.cpython-312-x86_64-linux-gnu.so', '-lcuda', '-L/home/flowingpurplecrane/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/backends/nvidia/lib', '-L/usr/lib/wsl/lib', '-L/lib/x86_64-linux-gnu', '-I/home/flowingpurplecrane/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp6415_gei', '-I/usr/include/python3.12']' returned non-zero exit status 1.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInductorError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/unsloth_zoo/vllm_utils.py:1771\u001b[39m, in \u001b[36mload_vllm\u001b[39m\u001b[34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, unsloth_vllm_standby, is_vision_model, return_args)\u001b[39m\n\u001b[32m   1770\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1771\u001b[39m     llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/entrypoints/llm.py:297\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:177\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m           \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m           \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m           \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m           \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m           \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:114\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m.logger_manager: Optional[StatLoggerManager] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:82\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInprocClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:245\u001b[39m, in \u001b[36mInprocClient.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/engine/core.py:92\u001b[39m, in \u001b[36mEngineCore.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, executor_fail_callback)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Setup KV Caches and update CacheConfig after profiling.\u001b[39;00m\n\u001b[32m     91\u001b[39m num_gpu_blocks, num_cpu_blocks, kv_cache_config = \\\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m vllm_config.cache_config.num_gpu_blocks = num_gpu_blocks\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/engine/core.py:190\u001b[39m, in \u001b[36mEngineCore._initialize_kv_caches\u001b[39m\u001b[34m(self, vllm_config)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# Profiles the peak memory usage of the model to determine how\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# much memory can be allocated for kv cache.\u001b[39;00m\n\u001b[32m    189\u001b[39m     available_gpu_memory = (\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetermine_available_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28mself\u001b[39m.available_gpu_memory_for_kv_cache = \\\n\u001b[32m    192\u001b[39m         available_gpu_memory[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/executor/abstract.py:85\u001b[39m, in \u001b[36mExecutor.determine_available_memory\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetermine_available_memory\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:  \u001b[38;5;66;03m# in bytes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdetermine_available_memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:83\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs, non_block)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m non_block:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/utils/__init__.py:3122\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   3121\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py:263\u001b[39m, in \u001b[36mWorker.determine_available_memory\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m memory_profiling(\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m.init_snapshot,\n\u001b[32m    261\u001b[39m         weights_memory=\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.model_runner.model_memory_usage),\n\u001b[32m    262\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m profile_result:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;28mself\u001b[39m.non_torch_memory = profile_result.non_torch_increase\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py:3392\u001b[39m, in \u001b[36mGPUModelRunner.profile_run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3390\u001b[39m \u001b[38;5;66;03m# Add `is_profile` here to pre-allocate communication buffers\u001b[39;00m\n\u001b[32m   3391\u001b[39m hidden_states, last_hidden_states \\\n\u001b[32m-> \u001b[39m\u001b[32m3392\u001b[39m     = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dummy_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_num_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_profile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py:3152\u001b[39m, in \u001b[36mGPUModelRunner._dummy_run\u001b[39m\u001b[34m(self, num_tokens, cudagraph_runtime_mode, force_attention, uniform_decode, allow_microbatching, skip_eplb, is_profile, create_mixed_batch, remove_lora)\u001b[39m\n\u001b[32m   3144\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_randomize_inputs(input_ids), set_forward_context(\n\u001b[32m   3145\u001b[39m         attn_metadata,\n\u001b[32m   3146\u001b[39m         \u001b[38;5;28mself\u001b[39m.vllm_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3150\u001b[39m         batch_descriptor=batch_descriptor,\n\u001b[32m   3151\u001b[39m         ubatch_slices=ubatch_slices):\n\u001b[32m-> \u001b[39m\u001b[32m3152\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3153\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3156\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3157\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_aux_hidden_state_outputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py:121\u001b[39m, in \u001b[36mCUDAGraphWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cudagraph_runtime_mode == CUDAGraphMode.NONE \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[32m    114\u001b[39m                     cudagraph_runtime_mode != \u001b[38;5;28mself\u001b[39m.runtime_mode:\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# CUDAGraphMode.NONE could mean the profile run, a warmup run, or\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# CUDAGraphWrapper when nesting multiple instances with different\u001b[39;00m\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# runtime modes.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_descriptor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.concrete_cudagraph_entries:\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# create a new entry for this batch descriptor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py:323\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, positions, intermediate_tensors, inputs_embeds)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    317\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    318\u001b[39m     input_ids: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    321\u001b[39m     inputs_embeds: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    322\u001b[39m ) -> Union[torch.Tensor, IntermediateTensors]:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m                               \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/vllm/compilation/decorators.py:310\u001b[39m, in \u001b[36m_support_torch_compile.<locals>.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m patch.object(\n\u001b[32m    305\u001b[39m         InliningInstructionTranslator, \u001b[33m\"\u001b[39m\u001b[33minline_call\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    306\u001b[39m         patched_inline_call), torch._dynamo.config.patch(\n\u001b[32m    307\u001b[39m             **dynamo_config_patches\n\u001b[32m    308\u001b[39m         ), maybe_use_cudagraph_partition_wrapper(\n\u001b[32m    309\u001b[39m             \u001b[38;5;28mself\u001b[39m.vllm_config), _torch27_patch_tensor_subclasses():\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompiled_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:749\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[32m    750\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    751\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:923\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe()).with_traceback(\n\u001b[32m    924\u001b[39m         e.__traceback__\n\u001b[32m    925\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:907\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m     mb_compiled_graph = \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:1578\u001b[39m, in \u001b[36mfx_codegen_and_compile\u001b[39m\u001b[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[39m\n\u001b[32m   1576\u001b[39m     scheme = _AsyncFxCompile(scheme)\n\u001b[32m-> \u001b[39m\u001b[32m1578\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:1456\u001b[39m, in \u001b[36m_InProcessFxCompile.codegen_and_compile\u001b[39m\u001b[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[39m\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1456\u001b[39m     compiled_module = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1457\u001b[39m     compiled_fn = compiled_module.call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/graph.py:2293\u001b[39m, in \u001b[36mGraphLowering.compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2287\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   2288\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGraphLowering.compile_to_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2289\u001b[39m     phase_name=\u001b[33m\"\u001b[39m\u001b[33mcode_gen\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2290\u001b[39m     log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2291\u001b[39m     dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2292\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/graph.py:2299\u001b[39m, in \u001b[36mGraphLowering._compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2295\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2296\u001b[39m     \u001b[38;5;66;03m# If we're here, we don't have to worry about the kernel code, which is only\u001b[39;00m\n\u001b[32m   2297\u001b[39m     \u001b[38;5;66;03m# returned separately in AOTInductor mode.\u001b[39;00m\n\u001b[32m   2298\u001b[39m     wrapper_code, _ = (\n\u001b[32m-> \u001b[39m\u001b[32m2299\u001b[39m         \u001b[38;5;28mself\u001b[39m.codegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2300\u001b[39m     )\n\u001b[32m   2302\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/graph.py:2238\u001b[39m, in \u001b[36mGraphLowering.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2237\u001b[39m \u001b[38;5;28mself\u001b[39m.wrapper_code.push_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2240\u001b[39m log.debug(\n\u001b[32m   2241\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished codegen for all nodes. The list of kernel names available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2242\u001b[39m     V.graph.all_codegen_kernel_names,\n\u001b[32m   2243\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/scheduler.py:4598\u001b[39m, in \u001b[36mScheduler.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4594\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mScheduler.codegen\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   4595\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   4596\u001b[39m         \u001b[38;5;28mself\u001b[39m._codegen_partitions()\n\u001b[32m   4597\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m torch._inductor.config.graph_partition\n\u001b[32m-> \u001b[39m\u001b[32m4598\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codegen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4599\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/scheduler.py:4750\u001b[39m, in \u001b[36mScheduler._codegen\u001b[39m\u001b[34m(self, nodes)\u001b[39m\n\u001b[32m   4749\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[32m-> \u001b[39m\u001b[32m4750\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4751\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/codegen/cuda_combined_scheduling.py:107\u001b[39m, in \u001b[36mCUDACombinedScheduling.codegen_node\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcodegen_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node: Union[FusedSchedulerNode, SchedulerNode]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_triton_scheduling\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/codegen/simd.py:1371\u001b[39m, in \u001b[36mSIMDScheduling.codegen_node\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m   1369\u001b[39m schedule_log.debug(\u001b[33m\"\u001b[39m\u001b[33mSchedule:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, node_schedule)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodegen_node_schedule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSIMDKernelFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoalesce_analysis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/codegen/simd.py:1424\u001b[39m, in \u001b[36mSIMDScheduling.codegen_node_schedule\u001b[39m\u001b[34m(self, kernel_features)\u001b[39m\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m V.set_kernel_handler(kernel):\n\u001b[32m-> \u001b[39m\u001b[32m1424\u001b[39m     src_code = \u001b[43mkernel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1425\u001b[39m kernel_name = \u001b[38;5;28mself\u001b[39m.define_kernel(src_code, node_schedule, kernel)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/codegen/triton.py:3677\u001b[39m, in \u001b[36mTritonKernel.codegen_kernel\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   3665\u001b[39m optimize_mem = V.graph.is_inference \u001b[38;5;129;01mor\u001b[39;00m V.graph.is_backward\n\u001b[32m   3667\u001b[39m inductor_meta = {\n\u001b[32m   3668\u001b[39m     \u001b[38;5;66;03m# Triton will not accept an OrderedSet for autotune_hints\u001b[39;00m\n\u001b[32m   3669\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgrid_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._get_grid_type().\u001b[34m__name__\u001b[39m,\n\u001b[32m   3670\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mautotune_hints\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.autotune_hints),  \u001b[38;5;66;03m# noqa: set_linter\u001b[39;00m\n\u001b[32m   3671\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mkernel_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(Placeholder.DESCRIPTIVE_NAME),\n\u001b[32m   3672\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmutated_arg_names\u001b[39m\u001b[33m\"\u001b[39m: mutated_args,\n\u001b[32m   3673\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptimize_mem\u001b[39m\u001b[33m\"\u001b[39m: optimize_mem,\n\u001b[32m   3674\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mno_x_dim\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.no_x_dim,\n\u001b[32m   3675\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_load\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.num_load,\n\u001b[32m   3676\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_reduction\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.num_reduction,\n\u001b[32m-> \u001b[39m\u001b[32m3677\u001b[39m     **\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minductor_meta_common\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   3678\u001b[39m }\n\u001b[32m   3679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tiling_scores:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/_inductor/codegen/triton.py:3501\u001b[39m, in \u001b[36mTritonKernel.inductor_meta_common\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3498\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   3499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minductor_meta_common\u001b[39m():\n\u001b[32m   3500\u001b[39m     inductor_meta = {\n\u001b[32m-> \u001b[39m\u001b[32m3501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbackend_hash\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_triton\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtriton_hash_with_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   3502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mare_deterministic_algorithms_enabled\u001b[39m\u001b[33m\"\u001b[39m: torch.are_deterministic_algorithms_enabled(),\n\u001b[32m   3503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33massert_indirect_indexing\u001b[39m\u001b[33m\"\u001b[39m: config.assert_indirect_indexing,\n\u001b[32m   3504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mautotune_local_cache\u001b[39m\u001b[33m\"\u001b[39m: config.autotune_local_cache,\n\u001b[32m   3505\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mautotune_pointwise\u001b[39m\u001b[33m\"\u001b[39m: config.triton.autotune_pointwise,\n\u001b[32m   3506\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mautotune_remote_cache\u001b[39m\u001b[33m\"\u001b[39m: config.autotune_remote_cache,\n\u001b[32m   3507\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_disable_caches\u001b[39m\u001b[33m\"\u001b[39m: config.force_disable_caches,\n\u001b[32m   3508\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdynamic_scale_rblock\u001b[39m\u001b[33m\"\u001b[39m: config.dynamic_scale_rblock,\n\u001b[32m   3509\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_autotune\u001b[39m\u001b[33m\"\u001b[39m: config.max_autotune,\n\u001b[32m   3510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_autotune_pointwise\u001b[39m\u001b[33m\"\u001b[39m: config.max_autotune_pointwise,\n\u001b[32m   3511\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmin_split_scan_rblock\u001b[39m\u001b[33m\"\u001b[39m: config.triton.min_split_scan_rblock,\n\u001b[32m   3512\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mspill_threshold\u001b[39m\u001b[33m\"\u001b[39m: config.triton.spill_threshold,\n\u001b[32m   3513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstore_cubin\u001b[39m\u001b[33m\"\u001b[39m: config.triton.store_cubin,\n\u001b[32m   3514\u001b[39m     }\n\u001b[32m   3515\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.version.hip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/utils/_triton.py:165\u001b[39m, in \u001b[36mtriton_hash_with_backend\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompiler\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompiler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triton_key\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m backend = \u001b[43mtriton_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtriton_key()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend.hash()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/torch/utils/_triton.py:157\u001b[39m, in \u001b[36mtriton_backend\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdriver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m driver\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m target = \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_current_target\u001b[49m()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m make_backend(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/runtime/driver.py:30\u001b[39m, in \u001b[36mLazyProxy.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/runtime/driver.py:26\u001b[39m, in \u001b[36mLazyProxy._initialize_obj\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28mself\u001b[39m._obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/runtime/driver.py:12\u001b[39m, in \u001b[36m_create_driver\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(active_drivers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m active drivers (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactive_drivers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). There should only be one.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactive_drivers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/backends/nvidia/driver.py:715\u001b[39m, in \u001b[36mCudaDriver.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m     \u001b[38;5;28mself\u001b[39m.utils = \u001b[43mCudaUtils\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO: make static\u001b[39;00m\n\u001b[32m    716\u001b[39m     \u001b[38;5;28mself\u001b[39m.launcher_cls = CudaLauncher\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/backends/nvidia/driver.py:62\u001b[39m, in \u001b[36mCudaUtils.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     mod = \u001b[43mcompile_module_from_src\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdriver.c\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda_utils\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_dirs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m.load_binary = mod.load_binary\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/runtime/build.py:88\u001b[39m, in \u001b[36mcompile_module_from_src\u001b[39m\u001b[34m(src, name, library_dirs, include_dirs, libraries)\u001b[39m\n\u001b[32m     87\u001b[39m     f.write(src)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m so = \u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibrary_dirs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(so, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/runtime/build.py:51\u001b[39m, in \u001b[36m_build\u001b[39m\u001b[34m(name, src, srcdir, library_dirs, include_dirs, libraries)\u001b[39m\n\u001b[32m     50\u001b[39m cc_cmd += [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m-I\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include_dirs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVNULL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m so\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:413\u001b[39m, in \u001b[36mcheck_call\u001b[39m\u001b[34m(*popenargs, **kwargs)\u001b[39m\n\u001b[32m    412\u001b[39m         cmd = popenargs[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[31mInductorError\u001b[39m: CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp6415_gei/cuda_utils.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmp6415_gei/cuda_utils.cpython-312-x86_64-linux-gnu.so', '-lcuda', '-L/home/flowingpurplecrane/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/backends/nvidia/lib', '-L/usr/lib/wsl/lib', '-L/lib/x86_64-linux-gnu', '-I/home/flowingpurplecrane/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp6415_gei', '-I/usr/include/python3.12']' returned non-zero exit status 1.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBASE_HUGGINGFACE_DIRECTORY\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBASE_MODEL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_SEQ_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# False for LoRA 16bit\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_VLLM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable vLLM fast inference\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLORA_RANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 8, 16, 32, ... (the larger the rank, the more memory it uses)\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGPU_MEMORY_UTILIZATION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduce if out of memory\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Disable \"think\" by monkey-patching the tokenizerâ€™s chat template\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# orig_apply = tokenizer.apply_chat_template\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# def apply_no_think(messages, **kwargs):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# tokenizer.apply_chat_template = apply_no_think\u001b[39;00m\n\u001b[32m     22\u001b[39m model = FastLanguageModel.get_peft_model(\n\u001b[32m     23\u001b[39m     model,\n\u001b[32m     24\u001b[39m     r=LORA_RANK,  \u001b[38;5;66;03m# Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     random_state=\u001b[32m3407\u001b[39m,\n\u001b[32m     38\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/unsloth/models/loader.py:482\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fast_inference:\n\u001b[32m    480\u001b[39m     fast_inference, model_name = fast_inference_setup(model_name, model_config)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m model, tokenizer = \u001b[43mdispatch_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    506\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/unsloth/models/qwen3.py:436\u001b[39m, in \u001b[36mFastQwen3Model.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained\u001b[39m(  \u001b[38;5;66;03m#TODO: Change after release\u001b[39;00m\n\u001b[32m    423\u001b[39m     model_name        = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen3-7B\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m     **kwargs,\n\u001b[32m    435\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastLlamaModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mFastQwen3Model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/unsloth/models/llama.py:2074\u001b[39m, in \u001b[36mFastLlamaModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, qat_scheme, **kwargs)\u001b[39m\n\u001b[32m   2071\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2073\u001b[39m \u001b[38;5;66;03m# Load vLLM first\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2074\u001b[39m llm = \u001b[43mload_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mload_vllm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[38;5;66;03m# Convert to HF format\u001b[39;00m\n\u001b[32m   2077\u001b[39m _, quant_state_dict = get_vllm_state_dict(llm, config = model_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/camhack2026/nerdle/nerl/lib/python3.12/site-packages/unsloth_zoo/vllm_utils.py:1796\u001b[39m, in \u001b[36mload_vllm\u001b[39m\u001b[34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, unsloth_vllm_standby, is_vision_model, return_args)\u001b[39m\n\u001b[32m   1791\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m   1792\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Retrying vLLM to process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapprox_max_num_seqs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sequences and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_num_batched_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens in tandem.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1793\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1794\u001b[39m             )\n\u001b[32m   1795\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1796\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error)\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1798\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp6415_gei/cuda_utils.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmp6415_gei/cuda_utils.cpython-312-x86_64-linux-gnu.so', '-lcuda', '-L/home/flowingpurplecrane/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/backends/nvidia/lib', '-L/usr/lib/wsl/lib', '-L/lib/x86_64-linux-gnu', '-I/home/flowingpurplecrane/camhack2026/nerdle/nerl/lib/python3.12/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp6415_gei', '-I/usr/include/python3.12']' returned non-zero exit status 1.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=f\"{BASE_HUGGINGFACE_DIRECTORY}/{BASE_MODEL}\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,  # False for LoRA 16bit\n",
    "    fast_inference=USE_VLLM,  # Enable vLLM fast inference\n",
    "    max_lora_rank=LORA_RANK,  # 8, 16, 32, ... (the larger the rank, the more memory it uses)\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,  # Reduce if out of memory\n",
    ")\n",
    "\n",
    "# Disable \"think\" by monkey-patching the tokenizerâ€™s chat template\n",
    "# orig_apply = tokenizer.apply_chat_template\n",
    "# def apply_no_think(messages, **kwargs):\n",
    "#     return orig_apply(\n",
    "#         messages,\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True,\n",
    "#         enable_thinking=False,      # â† hard-disable chain-of-thought\n",
    "#         **{k: v for k, v in kwargs.items() if k not in (\"tokenize\",\"add_generation_prompt\")}\n",
    "#     )\n",
    "# tokenizer.apply_chat_template = apply_no_think\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Remove QKVO if out of memory\n",
    "    lora_alpha=LORA_RANK,  # scaling parameter for delta W (LoRA) = alpha/rank ---> set same as rank so no need to retune for different rank\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Enable long context finetuning\n",
    "    # use_gradient_checkpointing= False, # Disable gradient checkpointing for faster training\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540079cd",
   "metadata": {},
   "source": [
    "## Garbage Collection\n",
    "\n",
    "Run this cell to clear gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbbfcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 1.53 GB\n",
      "Reserved: 1.54 GB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReserved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.memory_reserved(\u001b[32m0\u001b[39m)\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Full cleanup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m  \u001b[38;5;66;03m# Delete model object\u001b[39;00m\n\u001b[32m     13\u001b[39m gc.collect()\n\u001b[32m     14\u001b[39m torch.cuda.empty_cache()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA cache (if using GPU)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check GPU memory\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Full cleanup\n",
    "del model  # Delete model object\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3404b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# format of dataset:\n",
    "# {\"conversations\": [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": answer}]}\n",
    "# {\"conversations\": [{\"role\": \"user\", \"content\": prompt2}, {\"role\": \"assistant\", \"content\": answer2}]} ...\n",
    "dataset_original = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\").select(\n",
    "    range(TRAINSET_SIZE)\n",
    ")  # select first 1000 rows for training\n",
    "\n",
    "SYSTEM_INSTRUCTION = f\"\"\"\n",
    "You are a genius.\n",
    "\"\"\"\n",
    "\n",
    "# def formatting_prompts_func(example):\n",
    "#     # if number of tokens exceed max prompt length, do not include the prompt\n",
    "#     if len(tokenizer.encode(example[\"conversations\"][0][\"content\"])) < 10000:\n",
    "#         return {\n",
    "#             \"prompt\": [\n",
    "#                 {\"role\": \"user\", \"content\": SYSTEM_INSTRUCTION + \"\\n\" + example[\"conversations\"][0][\"content\"]}\n",
    "#             ],\n",
    "#             \"answer\": example[\"conversations\"][1][\"content\"]\n",
    "#         }\n",
    "#     else:\n",
    "#         # do not add this row to the new dataset object\n",
    "#         return None\n",
    "\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    # if number of tokens exceed max prompt length, do not include the prompt\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": SYSTEM_INSTRUCTION\n",
    "                + \"\\n\"\n",
    "                + example[\"conversations\"][0][\"content\"],\n",
    "            }\n",
    "        ],\n",
    "        \"answer\": example[\"conversations\"][1][\"content\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def is_short_enough(example):\n",
    "    return (\n",
    "        len(tokenizer.encode(example[\"conversations\"][0][\"content\"])) < 4096 - 150\n",
    "    ) and (len(tokenizer.encode(example[\"conversations\"][1][\"content\"])) < 4096 - 150)\n",
    "\n",
    "\n",
    "filtered_dataset = dataset_original.filter(is_short_enough)\n",
    "\n",
    "dataset_split = filtered_dataset.train_test_split(test_size=TESTSET_SIZE, seed=42)\n",
    "dataset_mapped = dataset_split.map(\n",
    "    formatting_prompts_func, remove_columns=[\"conversations\"]\n",
    ")\n",
    "# dataset_split = dataset_original.train_test_split(test_size=TESTSET_SIZE, seed=42)\n",
    "# dataset_mapped = dataset_split.map(formatting_prompts_func, remove_columns=[\"conversations\"])\n",
    "print(\"\\nOriginal dataset\\n\", dataset_original)\n",
    "print(\"\\nMapped dataset\\n\", dataset_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63445a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e850c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    use_vllm=USE_VLLM,  # use vLLM for fast inference!\n",
    "    learning_rate=5e-6,  # learning rate for gradient descent\n",
    "    adam_beta1=0.9,  # beta1 for adamw optimizer\n",
    "    adam_beta2=0.99,  # beta2 for adamw optimizer\n",
    "    weight_decay=0.1,  # weight decay for adamw optimizer\n",
    "    warmup_ratio=0.1,  # Gradually increases learning rate at the start\n",
    "    lr_scheduler_type=\"cosine\",  # After warmup, does cosine annealing on learning rate\n",
    "    optim=\"adamw_8bit\",  # Quantisation of adam update steps. This is fine and doesnt have huge impact on accuracy\n",
    "    per_device_train_batch_size=4,  # batch size per device\n",
    "    gradient_accumulation_steps=2,  # effective batch size is 256 = 64 * 4\n",
    "    num_generations=8,  # Number of outputs that GRPO produces to estimate the advantage\n",
    "    logging_steps=1,  # log every 4 steps\n",
    "    bf16=is_bfloat16_supported(),  # use bfloat16 for faster training\n",
    "    fp16=not is_bfloat16_supported(),  # if bf16 is not supported, use fp16\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,  # max length of the prompt\n",
    "    max_completion_length=MAX_COMPLETION_LENGTH,  # max length of the completion\n",
    "    num_train_epochs=EPOCHS,  # number of times to train over whole dataset\n",
    "    # max_steps = 1, # max number of steps\n",
    "    # save_steps = 250, # save every 250 steps\n",
    "    max_grad_norm=0.1,  # max gradient norm clipping to prevent exploding gradients\n",
    "    report_to=\"none\",  # Can use Weights & Biases\n",
    "    output_dir=\"outputs\",\n",
    "    loss_type=\"bnpo\",  # or \"grpo\" or \"dr_grpo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[reward_fn],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_mapped[\"train\"],\n",
    "    # eval_dataset = dataset_mapped[\"test\"],\n",
    ")\n",
    "# wandb.init(project=WEIGHTS_AND_BIASES_PROJECT_NAME,\n",
    "#            name=f\"{SAVE_MODEL_NAME}_rank{LORA_RANK}_epochs{EPOCHS}_{EXPERIMENT_DESCRIPTION}\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfcd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the training data\n",
    "logs = trainer.state.log_history\n",
    "\n",
    "# save the training logs\n",
    "df = pd.DataFrame(logs)\n",
    "df.to_csv(\n",
    "    f\"../logs/{SAVE_MODEL_NAME}_rank{LORA_RANK}_{EXPERIMENT_DESCRIPTION}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e035c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_merged(\n",
    "    f\"{SAVE_HUGGINGFACE_DIRECTORY}/{SAVE_MODEL_NAME}_rank{LORA_RANK}_{EXPERIMENT_DESCRIPTION}\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=HUGGINGFACE_TOKEN,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
